{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b351b865-eddc-4763-8180-c86b98d5fb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn==1.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06c6a31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1383f827-07d8-4c30-a9eb-9851407839e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.21\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8847ebf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved with sparse features\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load and sample\n",
    "df = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-03.parquet')\n",
    "df = df.dropna(subset=['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'trip_distance', 'PULocationID', 'DOLocationID'])\n",
    "df['duration'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "df = df[(df['duration'] >= 1) & (df['duration'] <= 60)]\n",
    "df['long_trip'] = (df['duration'] > 10).astype(int)\n",
    "\n",
    "# Sample data to reduce memory use during dev\n",
    "df = df.sample(n=100_000, random_state=1)\n",
    "\n",
    "features = ['PULocationID', 'DOLocationID', 'trip_distance']\n",
    "df[features] = df[features].astype(str)\n",
    "\n",
    "X_dict = df[features].to_dict(orient='records')\n",
    "y = df['long_trip']\n",
    "\n",
    "# Use sparse encoding\n",
    "dv = DictVectorizer(sparse=True)\n",
    "X = dv.fit_transform(X_dict)\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Save model\n",
    "with open('model.bin', 'wb') as f_out:\n",
    "    pickle.dump((dv, model), f_out)\n",
    "\n",
    "print(\"✅ Model saved with sparse features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb9877b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.bin', 'rb') as f_in:\n",
    "    dv, model = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04fdf9cc-32d6-4b94-8243-06fb29fd53e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['PULocationID', 'DOLocationID']\n",
    "\n",
    "def read_data(filename):\n",
    "    df = pd.read_parquet(filename)\n",
    "    \n",
    "    df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime\n",
    "    df['duration'] = df.duration.dt.total_seconds() / 60\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)].copy()\n",
    "\n",
    "    df[categorical] = df[categorical].fillna(-1).astype('int').astype('str')\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fffeb7d-0815-467c-8f3e-44faebb09e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_data('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-03.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f303223",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = df[categorical].to_dict(orient='records')\n",
    "X_val = dv.transform(dicts)\n",
    "y_pred = model.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f27d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'trip_distance', 'PULocationID', 'DOLocationID'])\n",
    "df['duration'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "df = df[(df['duration'] >= 1) & (df['duration'] <= 60)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3335bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already have a trained model (model) and vectorizer (dv)\n",
    "X_dict = df[features].to_dict(orient='records')\n",
    "X = dv.transform(X_dict)\n",
    "\n",
    "# Predict the trip durations\n",
    "predicted_durations = model.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa788a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '4'. Detailed error Yaml file '/home/anilm/mlops-zoomcamp/mlruns/4/meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anilm/anaconda3/envs/exp-lab2-env/lib/python3.9/site-packages/mlflow/store/tracking/file_store.py\", line 329, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"/home/anilm/anaconda3/envs/exp-lab2-env/lib/python3.9/site-packages/mlflow/store/tracking/file_store.py\", line 427, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/anilm/anaconda3/envs/exp-lab2-env/lib/python3.9/site-packages/mlflow/store/tracking/file_store.py\", line 1373, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/anilm/anaconda3/envs/exp-lab2-env/lib/python3.9/site-packages/mlflow/store/tracking/file_store.py\", line 1366, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/anilm/anaconda3/envs/exp-lab2-env/lib/python3.9/site-packages/mlflow/utils/file_utils.py\", line 310, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/anilm/mlops-zoomcamp/mlruns/4/meta.yaml' does not exist.\n",
      "WARNING:root:Malformed experiment '4'. Detailed error Yaml file '/home/anilm/mlops-zoomcamp/mlruns/4/meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anilm/anaconda3/envs/exp-lab2-env/lib/python3.9/site-packages/mlflow/store/tracking/file_store.py\", line 329, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"/home/anilm/anaconda3/envs/exp-lab2-env/lib/python3.9/site-packages/mlflow/store/tracking/file_store.py\", line 427, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/anilm/anaconda3/envs/exp-lab2-env/lib/python3.9/site-packages/mlflow/store/tracking/file_store.py\", line 1373, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/anilm/anaconda3/envs/exp-lab2-env/lib/python3.9/site-packages/mlflow/store/tracking/file_store.py\", line 1366, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/anilm/anaconda3/envs/exp-lab2-env/lib/python3.9/site-packages/mlflow/utils/file_utils.py\", line 310, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/anilm/mlops-zoomcamp/mlruns/4/meta.yaml' does not exist.\n",
      "2025/05/07 02:06:36 INFO mlflow.tracking.fluent: Experiment with name 'Lab5' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///home/anilm/mlops-zoomcamp/mlruns/374418875240430509', creation_time=1746583596441, experiment_id='374418875240430509', last_update_time=1746583596441, lifecycle_stage='active', name='Lab5', tags={}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_experiment(\"Lab5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f02d8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Standard Deviation of Predicted Durations: 6.76\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Load the data\n",
    "df = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-03.parquet')\n",
    "\n",
    "# Step 2: Clean and filter\n",
    "df['duration'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "df = df[(df['duration'] >= 1) & (df['duration'] <= 60)]\n",
    "\n",
    "# Step 3: Define features and target\n",
    "df['PULocationID'] = df['PULocationID'].astype(str)\n",
    "df['DOLocationID'] = df['DOLocationID'].astype(str)\n",
    "\n",
    "features = ['PULocationID', 'DOLocationID']\n",
    "X_dict = df[features].to_dict(orient='records')\n",
    "y = df['duration']\n",
    "\n",
    "# Step 4: Split the data\n",
    "X_train_dict, X_val_dict, y_train, y_val = train_test_split(X_dict, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Vectorize features\n",
    "dv = DictVectorizer()\n",
    "X_train = dv.fit_transform(X_train_dict)\n",
    "X_val = dv.transform(X_val_dict)\n",
    "\n",
    "# Step 6: Train the regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Predict on validation set and calculate std deviation\n",
    "y_pred = model.predict(X_val)\n",
    "std_dev = np.std(y_pred)\n",
    "\n",
    "print(f\"✅ Standard Deviation of Predicted Durations: {std_dev:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "003a76e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/07 02:09:56 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '0e7cadcfcb59422bab6073915514cc43', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
      "2025/05/07 02:09:56 WARNING mlflow.sklearn: Failed to log training dataset information to MLflow Tracking. Reason: Unable to allocate 12.8 GiB for an array with shape (3316216, 518) and data type float64\n",
      "2025/05/07 02:10:24 WARNING mlflow.sklearn: Failed to log evaluation dataset information to MLflow Tracking. Reason: Unable to allocate 12.8 GiB for an array with shape (3316216, 518) and data type float64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 65.46 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pyarrow as pa\n",
    "\n",
    "# -----------------------------\n",
    "# Load March data (validation)\n",
    "df_val = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-03.parquet')\n",
    "df_val['duration'] = (df_val['tpep_dropoff_datetime'] - df_val['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "df_val = df_val[(df_val['duration'] >= 1) & (df_val['duration'] <= 60)]\n",
    "df_val['PULocationID'] = df_val['PULocationID'].astype(str)\n",
    "df_val['DOLocationID'] = df_val['DOLocationID'].astype(str)\n",
    "\n",
    "# -----------------------------\n",
    "# Vectorization\n",
    "features = ['PULocationID', 'DOLocationID']\n",
    "\n",
    "dv = DictVectorizer()\n",
    "val_dicts = df_val[features].to_dict(orient='records')\n",
    "X_val = dv.fit_transform(val_dicts)\n",
    "y_val = df_val['duration']\n",
    "\n",
    "# -----------------------------\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X_val, y_val)\n",
    "\n",
    "# -----------------------------\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# -----------------------------\n",
    "# Create the results DataFrame\n",
    "df_result = pd.DataFrame({\n",
    "    'ride_id': [f'2023/03_{i}' for i in df_val.index],\n",
    "    'predicted_duration': y_pred\n",
    "})\n",
    "\n",
    "# -----------------------------\n",
    "# Save as Parquet\n",
    "output_file = 'predicted_durations_march_2023.parquet'\n",
    "df_result.to_parquet(\n",
    "    output_file,\n",
    "    engine='pyarrow',\n",
    "    compression=None,\n",
    "    index=False\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Check the file size\n",
    "import os\n",
    "file_size = os.path.getsize(output_file) / (1024 * 1024)  # In MB\n",
    "print(f\"File size: {file_size:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efcfcf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Mean Predicted Duration for 04/2023: 15.27 minutes\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "\n",
    "# Step 1: Define command-line arguments\n",
    "def parse_args():\n",
    "    if 'ipykernel' in sys.modules:  # Jupyter mode\n",
    "        year = 2023\n",
    "        month = 4\n",
    "        return year, month\n",
    "    else:\n",
    "        parser = argparse.ArgumentParser(description=\"Predict trip duration for a given year and month.\")\n",
    "        parser.add_argument('year', type=int, help='Year of the dataset (e.g., 2023)')\n",
    "        parser.add_argument('month', type=int, help='Month of the dataset (1-12)')\n",
    "        args = parser.parse_args()\n",
    "        return args.year, args.month\n",
    "\n",
    "# Step 2: Load and process the data\n",
    "def load_data(year, month):\n",
    "    file_url = f'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year:04d}-{month:02d}.parquet'\n",
    "    try:\n",
    "        df = pd.read_parquet(file_url)\n",
    "        df['duration'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "        df = df[(df['duration'] >= 1) & (df['duration'] <= 60)]\n",
    "        df['PULocationID'] = df['PULocationID'].astype(str)\n",
    "        df['DOLocationID'] = df['DOLocationID'].astype(str)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading data for {month}/{year}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Step 3: Train model and evaluate on test set\n",
    "def train_and_evaluate(df):\n",
    "    features = ['PULocationID', 'DOLocationID']\n",
    "    dv = DictVectorizer()\n",
    "    X_dict = df[features].to_dict(orient='records')\n",
    "    X = dv.fit_transform(X_dict)\n",
    "    y = df['duration']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mean_pred_duration = np.mean(y_pred)\n",
    "    return mean_pred_duration\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    year, month = parse_args()\n",
    "    df = load_data(year, month)\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"❌ No data available for prediction.\")\n",
    "        return\n",
    "\n",
    "    mean_pred_duration = train_and_evaluate(df)\n",
    "    print(f\"✅ Mean Predicted Duration for {month:02d}/{year}: {mean_pred_duration:.2f} minutes\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40829859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '4'. Detailed error Yaml file '/home/anilm/mlops-zoomcamp/mlruns/4/meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anilm/anaconda3/envs/exp-lab2-env/lib/python3.9/site-packages/mlflow/store/tracking/file_store.py\", line 329, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"/home/anilm/anaconda3/envs/exp-lab2-env/lib/python3.9/site-packages/mlflow/store/tracking/file_store.py\", line 427, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/anilm/anaconda3/envs/exp-lab2-env/lib/python3.9/site-packages/mlflow/store/tracking/file_store.py\", line 1373, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/anilm/anaconda3/envs/exp-lab2-env/lib/python3.9/site-packages/mlflow/store/tracking/file_store.py\", line 1366, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/anilm/anaconda3/envs/exp-lab2-env/lib/python3.9/site-packages/mlflow/utils/file_utils.py\", line 310, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/anilm/mlops-zoomcamp/mlruns/4/meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Experiment: artifact_location='file:///home/anilm/mlops-zoomcamp/mlruns/934363259442561227', creation_time=1746583642272, experiment_id='934363259442561227', last_update_time=1746583642272, lifecycle_stage='active', name='My_Lab5', tags={}>, <Experiment: artifact_location='file:///home/anilm/mlops-zoomcamp/mlruns/374418875240430509', creation_time=1746583596441, experiment_id='374418875240430509', last_update_time=1746583596441, lifecycle_stage='active', name='Lab5', tags={}>]\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# List all experiments\n",
    "experiments = mlflow.search_experiments()\n",
    "print(experiments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b064bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp-lab2-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
